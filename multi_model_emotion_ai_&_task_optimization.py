# -*- coding: utf-8 -*-
"""Multi-Model Emotion AI & Task Optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NxkKcukFFtDDaobq7YpXEHTSnA3gwVOG

#Installing requried Packages
"""

!pip install  librosa gradio transformers deepface fastapi loguru speechrecognition slack_sdk schedule

"""#Importing Requrierd Libraries for code Execution"""

# Import necessary libraries
from IPython import get_ipython  # Used for interacting with the IPython kernel
from IPython.display import display  # Used for displaying outputs in the notebook
import warnings  # For handling warnings

import os  # For interacting with the operating system (e.g., file paths)
import time  # For measuring execution time
import logging  # For logging messages
import sqlite3  # For working with SQLite databases

import torch  # PyTorch for deep learning
from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoFeatureExtractor, AutoModelForAudioClassification,AutoImageProcessor, AutoModelForImageClassification # For text , Speech & image emotion analysis  emotion analysis


import tensorflow as tf  # TensorFlow for deep learning
import librosa  # For audio processing

from PIL import Image  # For image processing

from fastapi import FastAPI, Request, HTTPException  # Import Request and HTTPException
from fastapi.staticfiles import StaticFiles  # For serving static files
from fastapi.responses import HTMLResponse  # For returning HTML responses

import gradio as gr  # For creating a user interface

import speech_recognition as sr  # For speech recognition
import smtplib  # For sending emails
import requests  # For making HTTP requests
from slack_sdk import WebClient  # For interacting with Slack
from slack_sdk.errors import SlackApiError  # For handling Slack API errors

from google.colab import userdata  # For accessing user data in Colab
access_token = userdata.get('Acces_Token')  # Get access token from user data
slack_bot_token = userdata.get('SLACK_BOT_TOKEN1')  # Get Slack bot token from user data
email_password = userdata.get('EMAIL_PASSWORD')  # Get email password from user data
Autentication_token = userdata.get('autentication_token')  # Get authentication token from user data
google_chat_webhook_url = "https://chat.google.com/room/AAAAWGCvkpo?cls=7"  # Define the Google Chat webhook URL

"""# Storing packages inside requiremnet file"""

import importlib.metadata

import subprocess

packages = [dist.metadata['Name'] for dist in importlib.metadata.distributions()]
with open('requirements.txt', 'w') as f:
     subprocess.call(['pip', 'freeze'], stdout=f)

"""#Loading models from google drive"""

from google.colab import drive  # For interacting with Google Drive
import os
import warnings
warnings.filterwarnings("ignore")
# Mount Google Drive
drive.mount('/content/drive')  # Mount Google Drive to access files
authentication=True
# Define the directory where you want to save the models
save_directory = "/content/drive/MyDrive/saved_models"  # Change this to your desired path

# Create the directory if it doesn't exist
if not os.path.exists(save_directory):  # Check if the directory exists
    os.makedirs(save_directory)  # Create the directory if it doesn't exist


def save_model(model, tokenizer_or_processor, model_name):
    """Saves the model and its associated tokenizer or processor."""
    model.save_pretrained(os.path.join(save_directory, model_name))  # Save the model
    if tokenizer_or_processor:  # Check if a tokenizer or processor is provided
      tokenizer_or_processor.save_pretrained(os.path.join(save_directory, model_name))  # Save the tokenizer or processor


try:
    text_tokenizer = AutoTokenizer.from_pretrained("j-hartmann/emotion-english-distilroberta-base")  # Load text tokenizer
    text_model = AutoModelForSequenceClassification.from_pretrained("j-hartmann/emotion-english-distilroberta-base",revision="main")  # Load text model
    text_model.eval()  # Set text model to evaluation mode
    save_model(text_model, text_tokenizer, "emotion-english-distilroberta-base")  # Save text model and tokenizer
    print("✅ Text Emotion Model Loaded and Saved Successfully!")  # Print success message
except Exception as e:
    print(f"❌ Error in Text Emotion Model: {e}")  # Print error message

  # Load your trained Swin Transformer model and processor
try:
      trained_model_path = "/content/extracted_files/MyDrive/2133/2133" # or wherever your facial model is saved
      facial_processor = AutoImageProcessor.from_pretrained(trained_model_path)
      facial_model = AutoModelForImageClassification.from_pretrained(trained_model_path)
      facial_model.eval()
      print("✅ Facial Emotion Model Loaded Successfully!")
except Exception as e:
  print(f"❌ Error Loading Facial Emotion Model: {e}")
  raise SystemExit("Stopping server due to model loading failure.")

try:
    # Load model directly
    from transformers import pipeline

    model_id = "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim"  # 7 emotions model

    processor = AutoFeatureExtractor.from_pretrained(model_id)
    speech_model = AutoModelForAudioClassification.from_pretrained(model_id)

    save_model(speech_model, processor, "language-classification")
    print("✅ Speech Emotion Model Loaded and Saved Successfully!")  # Print success message
except Exception as e:
    print(f"❌ Error in Loading Speech Emotion Model: {e}")  # Print error message

# prompt: write a code to print models label

labels = {} # You need to initialize labels to something, for example an empty dictionary
if hasattr(speech_model, 'config'):
  labels = speech_model.config.id2label
  print(labels)
else:
    print("Could not access labels in .speech_model.config.id2label")

import zipfile
import os

# Path to the zip file
zip_file_path = '/content/drive/MyDrive/2133.zip'

# Destination directory for extracted files
destination_directory = '/content/extracted_files/MyDrive/2133'

# Create the destination directory if it doesn't exist
os.makedirs(destination_directory, exist_ok=True)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(destination_directory)

print(f"Successfully unzipped {zip_file_path} to {destination_directory}")

"""#Creating a task based json file"""

import json

json_data = {
  "emotions": {
    "happy": {
      "general": [
        "Engage in creative brainstorming sessions",
        "Work on complex problem-solving tasks",
        "Initiate team collaboration and discussions",
        "Write reports, articles, or blogs",
        "Work on innovative projects or research",
        "Attend networking events",
        "Take leadership in a new project",
        "Host a webinar or knowledge-sharing session",
        "Mentor a colleague",
        "Plan future career development"
      ],

      "joy": list({  # Convert set to list using list()
          "Namaz phar",
          "Shukar kar",
      }),

      "IT": {
        "developer": [
          "Start a new coding project",
          "Experiment with a new programming language",
          "Contribute to open-source",
          "Write a technical blog",
          "Optimize system architecture",
          "Build a proof-of-concept"
        ],
        "designer": [
          "Create a new design concept",
          "Work on branding ideas",
          "Experiment with motion graphics",
          "Redesign an old UI/UX",
          "Create design system components",
          "Explore new design tools"
        ],
        "manager": [
          "Plan a team-building activity",
          "Strategize company growth",
          "Develop a long-term business plan",
          "Optimize workflow automation",
          "Improve team communication processes",
          "Plan skill development initiatives"
        ],
        "marketer": [
          "Run a new A/B testing campaign",
          "Brainstorm creative marketing ideas",
          "Host a live Q&A session",
          "Collaborate with influencers",
          "Analyze competitor strategies",
          "Plan a product launch campaign"
        ],
        "data_scientist": [
          "Explore new machine learning models",
          "Clean and organize datasets",
          "Visualize complex data",
          "Write research papers",
          "Optimize algorithms"
        ]
      }
    },
    "calm": {
      "general": [
        "Deep work on technical projects",
        "Data analysis, documentation, and reporting",
        "Studying or reviewing materials for learning",
        "Reviewing and optimizing past work",
        "Organizing workspace or digital files",
        "Plan long-term career or business goals",
        "Listen to industry-related podcasts",
        "Create to-do lists and schedules",
        "Review personal development plans"
      ],
      "IT": {
        "developer": [
          "Refactor old code",
          "Optimize database queries",
          "Write unit tests",
          "Improve code efficiency",
          "Document APIs",
          "Review system architecture"
        ],
        "designer": [
          "Work on UI/UX improvements",
          "Test color schemes",
          "Update design systems",
          "Enhance typography usage",
          "Create style guides",
          "Organize design assets"
        ],
        "manager": [
          "Conduct 1-on-1 meetings",
          "Review company metrics",
          "Assess team performance",
          "Update standard operating procedures",
          "Plan quarterly objectives",
          "Review budget allocations"
        ],
        "marketer": [
          "Analyze past marketing campaigns",
          "Plan a content calendar",
          "Review SEO strategies",
          "Develop lead generation plans",
          "Segment customer databases",
          "Analyze conversion funnels"
        ],
        "data_scientist": [
          "Clean and preprocess data",
          "Document analysis processes",
          "Review statistical models",
          "Create data visualizations",
          "Write technical documentation"
        ]
      }
    },
    "stressed": {
      "general": [
        "Break large tasks into smaller steps",
        "Prioritize urgent vs. important tasks",
        "Automate or delegate repetitive work",
        "Avoid multitasking and focus on one task",
        "Schedule mindfulness or relaxation breaks",
        "Step outside for a short walk",
        "Listen to calming music",
        "Practice deep breathing exercises"
      ],
      "IT": {
        "developer": [
          "Fix minor bugs instead of complex ones",
          "Review error logs",
          "Optimize simple scripts",
          "Work on documentation updates",
          "Write simple test cases",
          "Comment existing code"
        ],
        "designer": [
          "Work on simple design tasks",
          "Organize design files",
          "Fix minor visual inconsistencies",
          "Adjust existing layouts",
          "Create basic wireframes",
          "Review design guidelines"
        ],
        "manager": [
          "Delegate non-essential tasks",
          "Take a short break before major decisions",
          "Communicate transparently with the team",
          "Reassess immediate priorities",
          "Simplify meeting agendas",
          "Postpone non-critical discussions"
        ],
        "marketer": [
          "Schedule social media posts",
          "Respond to customer feedback",
          "Update analytics dashboards",
          "Review campaign budgets",
          "Organize marketing assets",
          "Draft simple email templates"
        ],
        "data_scientist": [
          "Clean small datasets",
          "Review existing analyses",
          "Document data sources",
          "Create simple visualizations",
          "Organize code repositories"
        ]
      }
    },
    "sad": {
      "general": [
        "Engage in low-effort, repetitive tasks",
        "Watch educational videos or tutorials",
        "Seek support from colleagues or mentors",
        "Work on personal development plans",
        "Listen to uplifting music or podcasts",
        "Write in a journal",
        "Take short, frequent breaks",
        "Connect with supportive coworkers"
      ],
      "IT": {
        "developer": [
          "Write documentation",
          "Organize code repositories",
          "Fix minor UI bugs",
          "Update API endpoints",
          "Review coding standards",
          "Test existing features"
        ],
        "designer": [
          "Review past work for inspiration",
          "Sketch simple ideas",
          "Adjust spacing and alignments",
          "Organize font libraries",
          "Browse design inspiration sites",
          "Update portfolio items"
        ],
        "manager": [
          "Review long-term business goals",
          "Check in with team morale",
          "Encourage team bonding activities",
          "Reflect on leadership growth",
          "Write encouraging notes to team",
          "Plan future team outings"
        ],
        "marketer": [
          "Revise old marketing content",
          "Look for new inspiration in industry trends",
          "Organize email marketing lists",
          "Respond to old customer queries",
          "Update media kits",
          "Review brand guidelines"
        ],
        "data_scientist": [
          "Organize datasets",
          "Review old analyses",
          "Document research methods",
          "Create simple charts",
          "Read industry papers"
        ]
      }
    },
    "angry": {
      "general": [
        "Step away from high-pressure work temporarily",
        "Work on independent, non-collaborative tasks",
        "Engage in a physical activity before resuming work",
        "Use structured workflows to reduce decision fatigue",
        "Practice breathing exercises",
        "Vent frustrations through journaling",
        "Listen to calming music",
        "Take a walk outside"
      ],
      "IT": {
        "developer": [
          "Refactor clean code",
          "Fix documentation issues",
          "Work on non-urgent bug fixes",
          "Explore new coding patterns",
          "Write technical documentation",
          "Review coding standards"
        ],
        "designer": [
          "Adjust minor design elements",
          "Organize layers in design software",
          "Update mood boards",
          "Rearrange design compositions",
          "Clean up design files",
          "Review design systems"
        ],
        "manager": [
          "Reassess workload distribution",
          "Avoid meetings for some time",
          "Draft improvement plans for team efficiency",
          "Analyze bottlenecks in workflows",
          "Document processes",
          "Review team structure"
        ],
        "marketer": [
          "Refine copywriting drafts",
          "Experiment with visual marketing",
          "Create automated reports",
          "Analyze competitor strategies",
          "Organize marketing assets",
          "Review brand messaging"
        ],
        "data_scientist": [
          "Clean data",
          "Document analysis processes",
          "Review statistical methods",
          "Organize code",
          "Write technical documentation"
        ]
      }
    },
    "fear": {
      "general": [
        "Break tasks into smaller steps",
        "Focus on learning and skill-building",
        "Seek guidance from mentors",
        "Practice positive self-talk",
        "Review past successes",
        "Create contingency plans",
        "Identify specific concerns",
        "Practice mindfulness techniques"
      ],
      "IT": {
        "developer": [
          "Work on well-defined small tasks",
          "Pair program with a colleague",
          "Review known working code",
          "Study documentation",
          "Write simple test cases",
          "Attend coding workshops"
        ],
        "designer": [
          "Work on familiar design patterns",
          "Review design principles",
          "Create low-fidelity prototypes",
          "Study successful designs",
          "Attend design critiques",
          "Practice design fundamentals"
        ],
        "manager": [
          "Focus on immediate priorities",
          "Seek advice from peers",
          "Review successful past projects",
          "Document decision processes",
          "Communicate concerns to superiors",
          "Break down strategic plans"
        ],
        "marketer": [
          "Analyze past successful campaigns",
          "Study market research",
          "Create small-scale tests",
          "Review customer personas",
          "Focus on proven channels",
          "Document marketing processes"
        ],
        "data_scientist": [
          "Work with familiar datasets",
          "Review basic statistics",
          "Document analysis steps",
          "Reproduce previous results",
          "Attend research seminars",
          "Practice with sample problems"
        ]
      }
    },
    "surprise": {
      "general": [
        "Pause to assess the situation",
        "Gather more information",
        "Consult with colleagues",
        "Adjust plans as needed",
        "Document lessons learned",
        "Maintain flexibility",
        "Identify opportunities",
        "Communicate changes clearly"
      ],
      "IT": {
        "developer": [
          "Research unexpected technical issues",
          "Consult documentation",
          "Reach out to more experienced developers",
          "Create fallback plans",
          "Document solutions for future reference",
          "Test alternative approaches"
        ],
        "designer": [
          "Research design trends",
          "Gather user feedback",
          "Create alternative design concepts",
          "Document design decisions",
          "Study competitor solutions",
          "Explore new design tools"
        ],
        "manager": [
          "Reassess project timelines",
          "Communicate with stakeholders",
          "Adjust resource allocations",
          "Document risk management strategies",
          "Conduct post-mortem analyses",
          "Update contingency plans"
        ],
        "marketer": [
          "Analyze market shifts",
          "Adjust campaign strategies",
          "Monitor social sentiment",
          "Document response strategies",
          "Test alternative messaging",
          "Review competitor responses"
        ],
        "data_scientist": [
          "Validate unexpected results",
          "Review data quality",
          "Consult statistical methods",
          "Document anomalies",
          "Test alternative models",
          "Seek peer review"
        ]
      }
    },
    "neutral": {
      "general": [
        "Focus on routine tasks",
        "Maintain consistent productivity",
        "Organize work environment",
        "Plan upcoming activities",
        "Review progress metrics",
        "Update documentation",
        "Attend to administrative tasks",
        "Prepare for future projects"
      ],
      "IT": {
        "developer": [
          "Write maintainable code",
          "Follow established patterns",
          "Attend standup meetings",
          "Review pull requests",
          "Update documentation",
          "Monitor system performance"
        ],
        "designer": [
          "Follow design systems",
          "Create production assets",
          "Attend design reviews",
          "Update style guides",
          "Prepare design handoffs",
          "Organize design files"
        ],
        "manager": [
          "Conduct routine meetings",
          "Review performance metrics",
          "Process approvals",
          "Update project plans",
          "Prepare reports",
          "Coordinate team activities"
        ],
        "marketer": [
          "Execute scheduled campaigns",
          "Monitor analytics",
          "Prepare routine reports",
          "Update content calendars",
          "Respond to standard inquiries",
          "Maintain marketing assets"
        ],
        "data_scientist": [
          "Run scheduled analyses",
          "Update dashboards",
          "Document processes",
          "Review data pipelines",
          "Monitor model performance",
          "Prepare routine reports"
        ]
      }
    }
  }
}

# Save the JSON data to file
file_path = "/content/drive/MyDrive/Task_json_file.json"
with open(file_path, 'w') as f:
    json.dump(json_data, f, indent=4)

print(f"JSON data successfully saved to: {file_path}")

"""#Starting and writing flask api logic"""

import os
import logging
import sys
from tensorflow.keras.models import load_model

# Ensure 'static' directory exists
if not os.path.exists("static"):
    os.makedirs("static")

# FastAPI App Setup
app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")

# Logger Setup
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

file_handler = logging.FileHandler("fastapi_logs.log")
file_handler.setLevel(logging.DEBUG)
file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(file_formatter)

console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(file_formatter)

logger.addHandler(file_handler)
logger.addHandler(console_handler)

logger.info("✅ FastAPI Server Starting...")

# Load Text Emotion Model
try:
    text_tokenizer = AutoTokenizer.from_pretrained("j-hartmann/emotion-english-distilroberta-base")
    text_model = AutoModelForSequenceClassification.from_pretrained("j-hartmann/emotion-english-distilroberta-base", revision="main")
    text_model.eval()
    logger.info("✅ Text Emotion Model Loaded Successfully!")
except Exception as e:
    logger.error(f"❌ Error in Text Emotion Model: {e}")
    raise SystemExit("Stopping server due to model loading failure.")

# Load Facial Emotion Model
try:
    facial_processor = AutoImageProcessor.from_pretrained("/content/extracted_files/MyDrive/2133/2133")
    facial_model = AutoModelForImageClassification.from_pretrained("/content/extracted_files/MyDrive/2133/2133")

    logger.info("✅ Facial Emotion Model Loaded Successfully!")
except Exception as e:
    logger.error(f"❌ Error in Facial Emotion Model: {e}")
    raise SystemExit("Stopping server due to model loading failure.")

# Load Task Recommendation Model
model_path = "/content/task_recommendation_model.keras"
try:
    task_recommendation_model = load_model(model_path)
    logger.info("✅ Task Recommendation Model Loaded Successfully!")
except Exception as e:
    logger.error(f"❌ Error in Task Recommendation Model: {e}")
    raise SystemExit("Stopping server due to model loading failure.")

# Test API Route
@app.get("/")
def home():
    return {"message": "FastAPI Emotion Detection Server is Running "}

"""# Security: Encryption Setup

"""

# Import Fernet for encryption
from cryptography.fernet import Fernet  # Import Fernet for encryption

# Configure logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

KEY_FILE = "secret.key"  # Define the key file name

def load_or_generate_key():
    """Loads an existing encryption key or generates a new one."""
    try:
        if os.path.exists(KEY_FILE):  # Check if the key file exists
            with open(KEY_FILE, "rb") as key_file:  # Open the key file in read binary mode
                key = key_file.read()  # Read the key from the file
            logger.info("Encryption key loaded successfully.")
        else:
            key = Fernet.generate_key()  # Generate a new key if the file doesn't exist
            with open(KEY_FILE, "wb") as key_file:  # Open the key file in write binary mode
                key_file.write(key)  # Write the key to the file
            logger.info("New encryption key generated and saved.")
        return key
    except Exception as e:
        logger.error(f"Error handling encryption key: {e}")
        raise

key = load_or_generate_key()
fernet = Fernet(key)  # Create a Fernet instance with the key

def encrypt_data(data: str) -> bytes:
    """Encrypts data using Fernet with error handling."""
    try:
        encrypted_data = fernet.encrypt(data.encode())  # Encrypt the data and return the encrypted bytes
        logger.info("Data encrypted successfully.")
        return encrypted_data
    except Exception as e:
        logger.error(f"Encryption failed: {e}")
        raise

def decrypt_data(token: bytes) -> str:
    """Decrypts data using Fernet with error handling."""
    try:
        decrypted_data = fernet.decrypt(token).decode()  # Decrypt the data and return the decrypted string
        logger.info("Data decrypted successfully.")
        return decrypted_data
    except Exception as e:
        logger.error(f"Decryption failed: {e}")
        raise

"""#  Database Setup for Feedback

"""

DB_FILE = "feedback.db"  # Define the database file name

def init_db():
    """Initializes the database and creates the feedback table if it does not exist."""
    try:
        conn = sqlite3.connect(DB_FILE, check_same_thread=False)  # Connect to the database
        cursor = conn.cursor()  # Create a cursor object
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS feedback (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                text_emotion TEXT,
                facial_emotion TEXT,
                speech_emotion TEXT,
                recommended_task TEXT,
                feedback TEXT
            )
        ''')  # Create feedback table if it does not exist
        conn.commit()  # Commit changes
        conn.close()  # Close connection
        logger.info("Database initialized successfully.")
    except Exception as e:
        logger.error(f"Error initializing database: {e}")
        raise

# Initialize the database
init_db()

"""# Setting up slack sdk"""

from slack_sdk.errors import SlackApiError
import smtplib  # Import smtplib

# Configure logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Securely Fetch Environment Variables
slack_bot_token = userdata.get('SLACK_BOT_TOKEN')
email_password = userdata.get('EMAIL_PASSWORD')

# Validate Slack token
if not slack_bot_token:  # Use slack_bot_token instead of slack_token
    logger.error("Missing Slack bot token! Set SLACK_BOT_TOKEN in environment variables.")
    raise ValueError("Missing SLACK_BOT_TOKEN in environment variables")

# Initialize Slack client
client = WebClient(token=slack_bot_token)  # Initialize Slack client

# Create and initialize the server object before using it
sender_email = "ghulammustafakeero@gmail.com"
server = smtplib.SMTP("smtp.gmail.com", 587)  # Create SMTP object
server.starttls()  # Start TLS encryption

# Now you can login
server.login(sender_email, email_password)  # Authenticate for email

def send_email_notification(to_email, subject, message):
    """Sends an email notification securely with error handling."""
    sender_email = "ghulammustafakeero@gmail.com"

    if not email_password:
        logger.error("Missing email password! Set EMAIL_PASSWORD in environment variables.")
        return

    try:
        server = smtplib.SMTP("smtp.gmail.com", 587)
        server.starttls()  # Secure connection
        server.login(sender_email, email_password)  # Authenticate

        # Set UTF-8 encoding for the email message
        email_message = f"Subject: {subject}\n\n{message}".encode('utf-8')  # Encode here

        server.sendmail(sender_email, to_email, email_message)
        logger.info(f"✅ Email sent successfully to {to_email}")
    except Exception as e:
        logger.error(f"❌ Error sending email: {e}")
    finally:
        server.quit()  # Ensure server connection is closed

"""#Setting up the falling and working machinesem of google chat"""

from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# Configure logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Securely Fetch Google Chat Webhook URL
Google_chat_webhook_url = google_chat_webhook_url

if not Google_chat_webhook_url:
    logger.error("⚠️ Missing Google Chat webhook URL! Set GOOGLE_CHAT_WEBHOOK_URL in environment variables.")

# Define a retry decorator for handling temporary API failures
@retry(
    stop=stop_after_attempt(3),  # Retry up to 3 times
    wait=wait_exponential(multiplier=1, min=1, max=4),  # Exponential backoff (1s, 2s, 4s)
    retry=retry_if_exception_type(requests.exceptions.RequestException),
    reraise=True  # Raise error after max retries
)
def send_google_chat_notification(message):
    """Sends a Google Chat notification securely with retry logic."""
    if not google_chat_webhook_url:
        logger.error("❌ Google Chat webhook URL is missing. Cannot send notification.")
        return

    headers = {"Content-Type": "application/json"}
    payload = {"text": message}

    try:
        response = requests.post(google_chat_webhook_url, json=payload, headers=headers)
        if response.status_code == 200:
            logger.info("✅ Google Chat message sent successfully!")
        else:
            logger.error(f"❌ Google Chat API error: {response.status_code}, {response.text}")
    except requests.exceptions.RequestException as e:
        logger.error(f"⚠️ Temporary network error: {e}. Retrying...")  # Log before retrying
        raise  # Trigger retry

def check_stress_and_notify(text_emotion, speech_emotion, facial_emotion):
    """Checks for stress indicators and sends notifications if detected."""
    negative_emotions = {"anger", "sadness", "fear", "disgust"}  # Use a set for faster lookup

    if {text_emotion.lower(), speech_emotion.lower(), facial_emotion.lower()} & negative_emotions:
        message = "🚨 Alert: Employee stress detected!"

        # Send notifications
        send_email_notification("gmkeeri26@gmail.com", "Stress Alert", message)
        send_slack_notification("#general", message)

        try:
            send_google_chat_notification(message)  # Now with retry logic
        except Exception:
            logger.error("❌ Google Chat notification failed after multiple attempts.")

        logger.warning(message)
    else:
        logger.info("✅ No stress detected.")

"""# Predicting the emotion"""

import torch
import torchaudio
from google.colab import files
from transformers import AutoModelForAudioClassification, AutoFeatureExtractor
import numpy as np

# 1. Try a model supporting more emotions (update this to match your 7 classes)
MODEL_NAME = "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim"  # 7 emotions model
EMOTION_MAP = {
    0: "neutral",
    1: "happy",
    2: "sad",
    3: "anger",
    4: "fear",
    5: "disgust",
    6: "surprise"
}

# Load model with trust_remote_code for custom architectures
feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME)
model = AutoModelForAudioClassification.from_pretrained(MODEL_NAME)

# 2. Audio Transformation Pipeline
def preprocess_audio(waveform, sample_rate):
    # Convert to mono
    if waveform.ndim > 1:
        waveform = waveform.mean(dim=0, keepdim=True)

    # Resample to model's expected rate
    if sample_rate != feature_extractor.sampling_rate:
        resampler = torchaudio.transforms.Resample(
            orig_freq=sample_rate,
            new_freq=feature_extractor.sampling_rate
        )
        waveform = resampler(waveform)

    # Add synthetic noise (helps for artificial voices)
    noise = torch.randn_like(waveform) * 0.005
    waveform += noise

    # Normalize loudness
    waveform = waveform / waveform.abs().max()

    return waveform.squeeze().numpy()

# 3. Process Uploaded File
uploaded = files.upload()
try:
    waveform, orig_sr = torchaudio.load(next(iter(uploaded)))
except Exception as e:
    raise ValueError(f"Audio loading failed: {str(e)}")

processed_audio = preprocess_audio(waveform, orig_sr)

# 4. Feature Extraction with Overlap
inputs = feature_extractor(
    processed_audio,
    sampling_rate=feature_extractor.sampling_rate,
    return_tensors="pt",
    padding="max_length",
    max_length=16000 * 6,  # 6-second chunks
    truncation=True,
    return_attention_mask=True
)

# 5. Inference with Temperature Scaling
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
inputs = {k: v.to(device) for k, v in inputs.items()}

with torch.no_grad():
    outputs = model(**inputs)
    # Temperature scaling for synthetic audio
    logits = outputs.logits / 0.3  # Adjust temperature (0.7-1.3)
    probs = torch.softmax(logits, dim=-1)

# 6. Post-processing
confidences = {EMOTION_MAP[i]: f"{p:.1%}"
              for i, p in enumerate(probs[0].cpu().numpy())}
pred_idx = probs.argmax().item()
pred_label = EMOTION_MAP.get(pred_idx, "unknown")

print(f"Predicted Emotion: {pred_label}")
print("Confidences:", confidences)

# Add to verify input quality
import matplotlib.pyplot as plt
plt.plot(processed_audio)
plt.title("Processed Audio Waveform")
plt.xlabel("Sample Index")
plt.ylabel("Amplitude")
plt.show()

import os
import json
import time
import torch
import numpy as np
import librosa
import cv2
from PIL import Image
import matplotlib.pyplot as plt
import logging

# Configure logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ----------------- Helper Functions -----------------

def save_model(model, processor, model_name, save_dir="./saved_models"):
    os.makedirs(save_dir, exist_ok=True)
    model_path = os.path.join(save_dir, model_name)
    model.save_pretrained(model_path)
    processor.save_pretrained(model_path)
    print(f"Model and processor saved to {model_path}")

def create_emotion_barplot(probabilities):
    """Create a barplot for emotion probabilities."""
    try:
        plt.figure(figsize=(8, 5))
        emotions = list(probabilities.keys())
        probs = list(probabilities.values())
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#B0A8B9', '#C0C0C0']
        bars = plt.bar(emotions, probs, color=colors[:len(emotions)])
        plt.xlabel("Emotions", fontsize=12)
        plt.ylabel("Probability", fontsize=12)
        plt.title("Emotion Prediction Probabilities", fontsize=14)
        plt.ylim(0, 1)
        plt.xticks(rotation=45, ha='right')
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height, f'{height:.2f}', ha='center', va='bottom')
        plot_path = "/content/emotion_barplot.png"
        plt.tight_layout()
        plt.savefig(plot_path, dpi=300)
        plt.close()
        return plot_path
    except Exception as e:
        logger.error(f"Error creating barplot: {e}")
        return None

# ----------------- Task Recommendation -----------------

# Emotion mapping: expected to be 7 classes for text & speech.
emotion_mapping = {0: "anger", 1: "disgust", 2: "fear", 3: "joy", 4: "neutral", 5: "sadness", 6: "surprise"}

# Load JSON data for task recommendations (adjust the path as needed)
with open("/content/drive/MyDrive/Task_json_file.json", "r") as f:
    task_data = json.load(f)

def get_task_recommendations(emotion, industry="general", json_data=None):
    """Retrieves task recommendations for a given emotion and industry."""
    if json_data is None:
        json_data = task_data
        logger.info("JSON data loaded successfully.")

    if emotion not in json_data["emotions"]:
        logger.warning(f"[WARNING] Emotion '{emotion}' not found in the JSON data.")
        return []

    if industry not in json_data["emotions"][emotion]:
        logger.warning(f"[WARNING] Industry '{industry}' not found for emotion '{emotion}' in the JSON data.")
        return []

    recommendations = json_data["emotions"][emotion].get(industry, [])
    if not recommendations:
        logger.warning(f"[WARNING] No recommendations found for emotion '{emotion}' in industry '{industry}'")
        return []

    return recommendations

# ----------------- Preprocessing Functions -----------------

def preprocess_text(text):
    """Preprocess text input for the text model."""
    try:
        inputs = text_tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        return inputs
    except Exception as e:
        logger.error(f"Text preprocessing error: {e}")
        return None

def preprocess_facial(image_input, video_frame=None):
    """Preprocess image input using the facial processor.
       Accepts an image file path or a video frame (numpy array).
    """
    try:
        if image_input is not None:
            image = Image.fromarray(image_input)
            inputs = facial_processor(images=image, return_tensors="pt")
        elif video_frame is not None:
            image = Image.fromarray(cv2.cvtColor(video_frame, cv2.COLOR_BGR2RGB))
            inputs = facial_processor(images=image, return_tensors="pt")
            return inputs
    except Exception as e:
        logger.error(f"Facial preprocessing error: {e}")
        return None

def preprocess_audio(audio_path):
    """Preprocess audio file for the speech model using its feature extractor."""
    try:
        audio, _ = librosa.load(audio_path, sr=16000)
        inputs = speech_feature_extractor(audio, sampling_rate=16000, return_tensors="pt")
        inputs = {key: value.to(speech_model.device) for key, value in inputs.items()}
        logger.info("Audio preprocessing completed.")
        return inputs
    except Exception as e:
        logger.error(f"Audio preprocessing error: {e}")
        return None

        return inputs
    except Exception as e:
      logger.error(f"Audio preprocessing error: {e}")
      return None

# ----------------- Prediction Function with Fusion -----------------

def predict_emotion(text, audio, image=None, video_frame=None):
    """
    Predict emotions from text, audio, and image/video inputs.
    Uses a fusion technique (summing logits from each modality) to determine the dominant emotion.
    """
    start_time = time.time()
    logger.info("Predicting emotions using fusion technique...")

    # Initialize variables
    text_emotion = "Unknown"
    facial_emotion = "Unknown"
    speech_emotion = "Unknown"
    barplot_path = None
    recommended_task = []

    # ---- Text Emotion Prediction ----
    try:
        text_inputs = preprocess_text(text)
        with torch.no_grad():
            text_outputs = text_model(**text_inputs)
        text_logits = text_outputs.logits  # Expected shape: (1, 7)
        text_pred_id = torch.argmax(text_logits, dim=1).item()
        text_emotion = emotion_mapping.get(text_pred_id, "Unknown")
        logger.info(f"Text Emotion: {text_emotion}")
    except Exception as e:
        text_emotion = "Error"
        text_logits = torch.zeros(1, len(emotion_mapping))
        logger.error(f"Text model error: {e}")

    # ---- Facial Emotion Prediction ----
    try:
        facial_inputs = preprocess_facial(image, video_frame)
        if facial_inputs is None:
            raise ValueError("Facial input preprocessing failed.")
        with torch.no_grad():
            facial_outputs = facial_model(**facial_inputs)
        facial_logits = facial_outputs.logits  # This may be of shape (1, X)
        # Check if facial logits match expected dimensions (7). If not, skip facial modality.
        if facial_logits.shape[1] != len(emotion_mapping):
            logger.warning(
                f"Facial logits dimension {facial_logits.shape[1]} does not match expected {len(emotion_mapping)}. "
                "Substituting with zeros for fusion."
            )
            facial_logits = torch.zeros(1, len(emotion_mapping))
            facial_pred_id = 0  # Default to neutral
            facial_emotion = "Neutral"  # Default to neutral emotion
        else:
            facial_pred_id = torch.argmax(facial_logits, dim=1).item()
            facial_emotion = emotion_mapping.get(facial_pred_id, "Unknown")

        logger.info(f"Facial Emotion: {facial_emotion}")
    except Exception as e:
        facial_emotion = "Error"
        facial_logits = torch.zeros(1, len(emotion_mapping))
        logger.error(f"Facial model error: {e}")

    # ---- Speech Emotion Prediction ----
    try:
        audio_inputs = preprocess_audio(audio)
        if audio_inputs is None:
            raise ValueError("Audio input preprocessing failed.")
        with torch.no_grad():
            speech_outputs = speech_model(**audio_inputs)
        speech_logits = speech_outputs.logits  # Expected shape: (1, 7)
        speech_pred_id = torch.argmax(speech_logits, dim=1).item()
        speech_emotion = emotion_mapping.get(speech_pred_id, "Unknown")
        logger.info(f"Speech Emotion: {speech_emotion}")
    except Exception as e:
        speech_emotion = "Error"
        speech_logits = torch.zeros(1, len(emotion_mapping))
        logger.error(f"Speech model error: {e}")

    try:
        # Map facial emotion string to a numerical id
        emotion_to_id = {emotion: idx for idx, emotion in enumerate(emotion_mapping.values())}
        predicted_class_id = text_pred_id if (text_pred_id is not None) else -1
        logger.debug(f"Predicted Class ID: {predicted_class_id}")
        logger.debug(f"Speech Emotion ID: {speech_emotion}")
        logger.debug(f"Facial Emotion ID: {facial_emotion}")
        logger.debug(f"Emotion to ID Mapping: {emotion_to_id}")

        speech_emotion_id = emotion_to_id.get(speech_emotion.lower(), -1)
        logger.debug(f"Speech Emotion ID: {speech_emotion_id}")
        facial_emotion_id = emotion_to_id.get(facial_emotion.lower(), -1)
        logger.debug(f"Facial Emotion ID: {facial_emotion_id}")

        task_input = [text_emotion, speech_emotion_id, facial_emotion_id]
        logger.debug(f"Task Input: {task_input}")
        task_recommendation_model = get_task_recommendations(text_emotion, json_data=task_data)
        logger.debug(f"Task Recommendation Model: {task_recommendation_model}")
        # Check if task recommendation model is available
        if task_recommendation_model is not None:
            recommended_task_pred = task_recommendation_model.predict([task_input])

            logger.debug(f"Task Prediction: {recommended_task_pred}")
            if recommended_task_pred is not None:
                recommended_task = recommended_task_pred[0]
                logger.info(f"Recommended Task: {recommended_task}")
        else:
            logger.warning("No task recommendation model available.")

        total_time = time.time() - start_time
        logger.info(f"Total Prediction Time: {total_time:.4f} seconds")
        # Check for stress and send notifications if needed
        check_stress_and_notify(text_emotion, speech_emotion, facial_emotion)
        return text_emotion, speech_emotion, facial_emotion, recommended_task
    except Exception as e:
        logger.error(f"Prediction Error: {e}")
        return "Error", "Error", "Error", "Error"

    # ---- Fusion ----
    try:
        fusion_logits = text_logits + speech_logits + facial_logits
        fusion_pred_id = torch.argmax(fusion_logits, dim=1).item()
        fusion_emotion = emotion_mapping.get(fusion_pred_id, "Unknown")
        logger.info(f"Fusion Emotion: {fusion_emotion}")
        return fusion_emotion

    except Exception as e:
        logger.error(f"Fusion Error: {e}")
        return "Error"

"""# Based on feedback improve the model"""

@app.post("/feedback")
async def feedback_endpoint(request: Request):
    data = await request.json()
    timestamp = time.strftime("%Y-%m-%d %H:%M:%S")

    text_emotion = data.get("text_emotion", "Unknown")
    facial_emotion = data.get("facial_emotion", "Unknown")
    speech_emotion = data.get("speech_emotion", "Unknown")
    recommended_task = data.get("recommended_task", "Unknown")
    corrected_emotion = data.get("corrected_emotion", None)  # New field

    encrypted_feedback = encrypt_data(data["feedback"])
    feedback_value = data["feedback"]
    logger.info(f"Received feedback: {feedback_value}")
    logger.info(f"Encrypted feedback: {encrypted_feedback}")
    logger.info(f"Corrected emotion: {corrected_emotion}")

    try:
        cursor.execute("""
            INSERT INTO feedback (timestamp, text_emotion, facial_emotion, speech_emotion, recommended_task, feedback, corrected_emotion)
            VALUES (?, ?, ?, ?, ?, ?, ?)""",
            (timestamp, text_emotion, facial_emotion, speech_emotion, recommended_task, encrypted_feedback, corrected_emotion)
        )
        conn.commit()
        logger.info("Feedback stored successfully.")

        if feedback_value == "negative" and corrected_emotion:
            store_mislabeled_data(text_emotion, speech_emotion, facial_emotion, corrected_emotion)

        return {"status": "success", "message": "Feedback stored and used for model improvement."}
    except Exception as e:
        logger.error(f"Error storing feedback: {e}")
        return {"status": "error", "message": "Failed to store feedback."}

"""# Storing mislabeld data

"""

import json

def store_mislabeled_data(text_emotion, speech_emotion, facial_emotion, corrected_emotion):
    mislabeled_data = {
        "text_emotion": text_emotion,
        "speech_emotion": speech_emotion,
        "facial_emotion": facial_emotion,
        "corrected_emotion": corrected_emotion
    }
    with open("mislabeled_data.json", "a") as f:
        f.write(json.dumps(mislabeled_data) + "\n")

    logger.info(f"Stored incorrect prediction for retraining: {mislabeled_data}")

"""# Retrain our model"""

import torch

def retrain_model():
    try:
        with open("mislabeled_data.json", "r") as f:
            feedback_data = [json.loads(line) for line in f]

        if not feedback_data:
            logger.info("No feedback data available for retraining.")
            return

        # Convert feedback data to a format suitable for training
        X_train, y_train = [], []
        for data in feedback_data:
            X_train.append([data["text_emotion"], data["speech_emotion"], data["facial_emotion"]])
            y_train.append(data["corrected_emotion"])

        # Convert to PyTorch tensors
        X_train = torch.tensor(X_train, dtype=torch.float32)
        y_train = torch.tensor(y_train, dtype=torch.long)

        # Fine-tune the existing model
        model.train()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        loss_fn = torch.nn.CrossEntropyLoss()

        for epoch in range(5):  # Retrain for 5 epochs
            optimizer.zero_grad()
            outputs = model(X_train)
            loss = loss_fn(outputs, y_train)
            loss.backward()
            optimizer.step()
            logger.info(f"Retraining epoch {epoch+1}: Loss = {loss.item()}")

        # Save the retrained model
        model_path = "retrained_emotion_model.pth"
        torch.save(model.state_dict(), model_path)
        logger.info(f"Retrained model saved at {model_path}")

        # Update model in FastAPI app
        update_model(model_path)

    except Exception as e:
        logger.error(f"Error during retraining: {e}")

def update_model(model_path="retrained_emotion_model.pth"):
    global model
    model.load_state_dict(torch.load(model_path))
    model.eval()
    logger.info("Updated FastAPI with the new retrained model.")

"""# Based on feed back by user model correct it self by a specific shedulae"""

import schedule
import time
import threading

schedule.every(1).days.do(retrain_model)

def run_scheduler():
    while True:
        schedule.run_pending()
        time.sleep(60)

# Run in a separate thread

scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
scheduler_thread.start()

"""# 2nd P_Emotion function for handling internal functions"""

# prompt: Based on my predict logic generate a internal predication api logic by using websockets and what ever possibel

import asyncio
import websockets
import json
from typing import Dict, Any
import threading
import nest_asyncio
nest_asyncio.apply()

async def internal_prediction_api(websocket, path):
  """Handles WebSocket connections for internal prediction requests."""
  try:
    async for message in websocket:
      try:
        data = json.loads(message)

        # Extract necessary data from the request
        text = data.get("text", "")
        audio = data.get("audio", None)
        image = data.get("image", None)
        video_frame = data.get("video_frame", None)

        # Make the prediction using your existing predict_emotion function
        text_emotion, speech_emotion, facial_emotion, recommended_task, barplot_path = predict_emotion(text, audio, image, video_frame)

        # Construct the response
        response = {
            "text_emotion": text_emotion,
            "speech_emotion": speech_emotion,
            "facial_emotion": facial_emotion,
            "recommended_task": recommended_task,
            "barplot_path": barplot_path
        }

        # Send the response back to the client via WebSocket
        await websocket.send(json.dumps(response))

      except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON: {e}")
        await websocket.send(json.dumps({"error": "Invalid JSON format"}))
      except Exception as e:
        logger.error(f"Error during internal prediction: {e}")
        await websocket.send(json.dumps({"error": "Prediction failed"}))

  except websockets.exceptions.ConnectionClosedOK:
    logger.info("WebSocket connection closed cleanly.")
  except Exception as e:
    logger.error(f"WebSocket error: {e}")


async def start_internal_api():
  """Starts the internal prediction API using websockets."""
  async with websockets.serve(internal_prediction_api, "0.0.0.0", 8765):
    await asyncio.Future()  # Run forever
    logger.info("WebSocket server started.")

# Start the WebSocket server in a separate thread
loop = asyncio.new_event_loop()
asyncio.set_event_loop(loop)
internal_api_thread = threading.Thread(target=loop.run_until_complete, args=(start_internal_api(),), daemon=True)
internal_api_thread.start()

import sqlite3
import datetime
import random

def generate_sample_data(num_entries=100):
    conn = sqlite3.connect('team_mood.db')
    cursor = conn.cursor()

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS team_mood (
            timestamp TEXT,
            team TEXT,
            average_mood REAL,
            stress_level INTEGER
        )
    ''')

    teams = ['Green', 'White', 'Red', 'Blue']
    for _ in range(num_entries):
        timestamp = datetime.datetime.now() - datetime.timedelta(days=random.randint(0, 30))
        timestamp = timestamp.strftime('%Y-%m-%d %H:%M:%S')
        team = random.choice(teams)
        average_mood = random.uniform(1, 5)  # Mood on a scale of 1-5
        stress_level = random.randint(0, 10)  # Stress level from 0-10

        cursor.execute('''
            INSERT INTO team_mood (timestamp, team, average_mood, stress_level)
            VALUES (?, ?, ?, ?)
        ''', (timestamp, team, average_mood, stress_level))

    conn.commit()
    conn.close()

generate_sample_data()
logger.info("Sample data generated successfully.")

"""# Visulizing the data"""

import plotly.express as px
from sklearn.ensemble import IsolationForest
from scipy.stats import zscore
import pandas as pd
import numpy as np

# Sample Team Data with Department Column
df_dashboard = pd.DataFrame({
    "timestamp": pd.date_range(start="2025-01-01", periods=50, freq="D"),
    "team": np.random.choice(["Data Sciecne", "Machine Learning ", "Computer Visison"], 50),
    "department": np.random.choice(["Engineering", "Sales", "HR"], 50),
    "average_mood": np.random.uniform(0, 1, 50),
    "stress_level": np.random.uniform(0, 1, 50)
})

# Compute Rolling Averages for Trend Analysis
df_dashboard['mood_trend'] = df_dashboard.groupby('team')['average_mood'].transform(lambda x: x.rolling(window=5, min_periods=1).mean())
df_dashboard['stress_trend'] = df_dashboard.groupby('team')['stress_level'].transform(lambda x: x.rolling(window=5, min_periods=1).mean())

# Anomaly Detection with Z-Score
df_dashboard['stress_zscore'] = zscore(df_dashboard['stress_level'])
df_dashboard['anomaly_alert'] = df_dashboard['stress_zscore'].apply(lambda x: "High" if abs(x) > 2 else "Low")

# Train Burnout Prediction Model
burnout_model = IsolationForest(contamination=0.1)
df_dashboard["burnout_risk"] = burnout_model.fit_predict(df_dashboard[["average_mood", "stress_level"]])
df_dashboard["burnout_risk"] = df_dashboard["burnout_risk"].apply(lambda x: "High" if x == -1 else "Low")

@app.get("/dashboard", response_class=HTMLResponse)
async def dashboard():
    fig = px.line(df_dashboard, x="timestamp", y="stress_trend", color="team", title="Stress Trends per Team")
    fig2 = px.scatter(df_dashboard, x="timestamp", y="stress_level", color="anomaly_alert", title="Anomaly Detection Alerts")
    fig3 = px.bar(df_dashboard, x="department", y="average_mood", color="burnout_risk", title="Burnout Risk by Department")

    return f"{fig.to_html(full_html=False)}{fig2.to_html(full_html=False)}{fig3.to_html(full_html=False)}"

"""#Live video camera detection"""

from google.colab import output
import time
import base64
from io import BytesIO
import numpy as np
from deepface import DeepFace

def capture_image():
    """Capture an image from the webcam in Google Colab and return Base64 image data."""
    js = """
    async function captureImage() {
        return new Promise(async (resolve, reject) => {
            try {
                const video = document.createElement('video');
                document.body.appendChild(video);

                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                await video.play();

                const canvas = document.createElement('canvas');
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                const ctx = canvas.getContext('2d');

                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                stream.getTracks().forEach(track => track.stop());
                document.body.removeChild(video);

                resolve(canvas.toDataURL('image/png'));
            } catch (error) {
                reject(error);
            }
        });
    }
    captureImage();
    """
    try:
        image_data = output.eval_js(js)  # Capture image via JavaScript
        if not image_data:
            print("🔴 No image data received from JavaScript function.")
        return image_data
    except Exception as e:
        print(f"⚠️ JavaScript Error: {e}")
        return None

# **Run Continuous Emotion Detection**
while True:
  try:
    image_data = capture_image()
    if not image_data:
      print("🔴 Failed to capture image. Retrying in 3 seconds...")
      time.sleep(3)
      continue  # Skip to next iteration
      # Convert image to numpy array
      image_bytes = base64.b64decode(image_data.split(',')[1])
      image_np = np.array(PIL.Image.open(BytesIO(image_bytes)))
      image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)
      image_np = cv2.resize(image_np, (224, 224))

      # Run Emotion Detection
      deepface_result = DeepFace.analyze(
          img_path=image_np,
          actions=['emotion'],
          enforce_detection=False,
          silent=True  # Prevents unnecessary warnings
          )

      if deepface_result:
        predicted_emotion = deepface_result[0]['dominant_emotion']
        print(f"✅ Detected Emotion: {predicted_emotion}")
      else:
        print("⚠️ No face detected in the image.")

        # Display the image with a label
        img_pil = PIL.Image.fromarray(image_np)
        IPython.display.clear_output(wait=True)  # Clear previous output
        IPython.display.display(img_pil)
        print(f"🟢 Predicted Emotion: {predicted_emotion}")

        time.sleep(4)  # Delay before next capture
  except Exception as e:
    print(f"⚠️ Error: {e}")
    time.sleep(3)  # Delay before next capture

"""# Gradio UI"""

import gradio as gr

iface = gr.Interface(
    fn=predict_emotion,
    inputs=[gr.Textbox(label="Input Text"),
            gr.Audio(type="filepath", label="Input Audio"),
            gr.Image(type="numpy", label="Input Image"),
            ],

    outputs=[
        gr.Textbox(label="Text Emotion"),
        gr.Textbox(label="Speech Emotion"),
        gr.Textbox(label="Facial Emotion"),
        gr.Textbox(label="Recommended Task"),
        gr.Image(label="Emotion Distribution"),
    ],

)

iface.launch(debug=True,share=True)

"""## Future Work"""

async function get_output_values() {
    const text_emotion = document.getElementById("text_emotion_output").innerText;
    const speech_emotion = document.getElementById("speech_emotion_output").innerText;
    const facial_emotion = document.getElementById("facial_emotion_output").innerText;
    const recommended_task = document.getElementById("recommended_task_output").innerText;

    const output_data = {
        text_emotion: text_emotion,
        speech_emotion: speech_emotion,
        facial_emotion: facial_emotion,
        recommended_task: recommended_task
    };

    google.colab.kernel.invokeFunction('process_output', [output_data], {});
}

from google.colab import output

def process_output(output_data):
    text_emotion = output_data['text_emotion']
    speech_emotion = output_data['speech_emotion']
    facial_emotion = output_data['facial_emotion']
    recommended_task = output_data['recommended_task']

    print(f"Text Emotion: {text_emotion}")
    print(f"Speech Emotion: {speech_emotion}")
    print(f"Facial Emotion: {facial_emotion}")
    print(f"Recommended Task: {recommended_task}")

output.register_callback('process_output', process_output)

